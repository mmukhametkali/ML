{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learnining: Lab and HW 10\n",
    "### Homework Tasks:\n",
    "* Plot the error\n",
    "* Model XOR with the help of sigmoid, linear\n",
    "* Add moments rule to learning equation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "k = 1\n",
    "def sigmoid(x):\n",
    "    return 1.0/(1.0 + np.exp(-k*x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    p1= sigmoid(x)*(1.0-sigmoid(x))\n",
    "    return p1 * (1-p1)\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_prime(x):\n",
    "    return 1.0 - x**2\n",
    "\n",
    "def linear(x):\n",
    "    return x\n",
    "\n",
    "def linear_prime(x):\n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.80818659 -0.98796293 -0.70490432]\n",
      " [ 0.19537285  0.49945386  0.9971818 ]\n",
      " [-0.94007033 -0.54841139 -0.73090043]]\n",
      "[[-0.24845753]\n",
      " [ 0.95469956]\n",
      " [-0.22241829]]\n",
      "[0 0] [-0.98722422]\n",
      "[0 1] [-1.11465915]\n",
      "[1 0] [-0.78072916]\n",
      "[1 1] [-0.9081641]\n",
      "[[1. 0. 0.]\n",
      " [1. 0. 1.]\n",
      " [1. 1. 0.]\n",
      " [1. 1. 1.]]\n",
      "epochs: 0\n",
      "epochs: 10000\n",
      "epochs: 20000\n",
      "epochs: 30000\n",
      "epochs: 40000\n",
      "epochs: 50000\n",
      "epochs: 60000\n",
      "epochs: 70000\n",
      "epochs: 80000\n",
      "epochs: 90000\n",
      "[0 0] [-0.98722422]\n",
      "[0 1] [-1.11465915]\n",
      "[1 0] [-0.78072916]\n",
      "[1 1] [-0.9081641]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class NeuralNetwork:\n",
    "\n",
    "    def __init__(self, layers):\n",
    "#        self.activation = tanh\n",
    "#       self.activation_prime = tanh_prime\n",
    "        self.activation = linear\n",
    "        self.activation_prime = linear_prime\n",
    "#         self.activation = sigmoid\n",
    "#         self.activation_prime = sigmoid_prime\n",
    "        \n",
    "        # Set weights\n",
    "        self.weights = []\n",
    "        # layers = [2,2,1]\n",
    "        # range of weight values (-1,1)\n",
    "        # input and hidden layers - random((2+1, 2+1)) : 3 x 3\n",
    "        #              1, 2\n",
    "        for i in range(1, len(layers) - 1):\n",
    "            r = 2*np.random.random((layers[i-1] + 1, layers[i] + 1)) -1\n",
    "            self.weights.append(r)\n",
    "            print(r)\n",
    "        # output layer - random((2+1, 1)) : 3 x 1\n",
    "        r = 2*np.random.random( (layers[i] + 1, layers[i+1])) - 1\n",
    "        print(r)\n",
    "        self.weights.append(r)\n",
    "\n",
    "    def fit(self, X, y, learning_rate=0.2, epochs=100000):\n",
    "        # Add column of ones to X\n",
    "        # This is to add the bias unit to the input layer\n",
    "        ones = np.atleast_2d(np.ones(X.shape[0]))\n",
    "        X = np.concatenate((ones.T, X), axis=1)\n",
    "        print(X)\n",
    "        epochs_X = np.arange(epochs)\n",
    "        error_Y = []\n",
    "        for k in range(epochs):\n",
    "            i = np.random.randint(X.shape[0])\n",
    "            a = [X[i]]\n",
    "\n",
    "            for l in range(len(self.weights)):\n",
    "                    dot_value = np.dot(a[l], self.weights[l])\n",
    "                    activation = self.activation(dot_value)\n",
    "                    a.append(activation)\n",
    "            # output layer\n",
    "\n",
    "            error = y[i] - a[-1]\n",
    "            error_Y.append(error)\n",
    "            deltas = [error * self.activation_prime(a[-1])]\n",
    "\n",
    "            # we need to begin at the second to last layer \n",
    "            # (a layer before the output layer)\n",
    "            for l in range(len(a) - 2, 0, -1): \n",
    "                deltas.append(deltas[-1].dot(self.weights[l].T)*self.activation_prime(a[l]))\n",
    "\n",
    "            # reverse\n",
    "            # [level3(output)->level2(hidden)]  => [level2(hidden)->level3(output)]\n",
    "            deltas.reverse()\n",
    "\n",
    "            # backpropagation\n",
    "            # 1. Multiply its output delta and input activation \n",
    "            #    to get the gradient of the weight.\n",
    "            # 2. Subtract a ratio (percentage) of the gradient from the weight.\n",
    "            for i in range(len(self.weights)):\n",
    "                layer = np.atleast_2d(a[i])\n",
    "                delta = np.atleast_2d(deltas[i])\n",
    "                self.weights[i] += learning_rate * layer.T.dot(delta)\n",
    "\n",
    "            if k % 10000 == 0: \n",
    "                print('epochs:', k)\n",
    "        plt.plot(epochs_X, error_Y, '*')\n",
    "\n",
    "    def predict(self, x): \n",
    "    \n",
    "        a = np.concatenate((np.ones(1).T, np.array(x)))      \n",
    "        \n",
    "\n",
    "        for l in range(0, len(self.weights)):\n",
    "            a = self.activation(np.dot(a, self.weights[l]))\n",
    "        return a\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    nn = NeuralNetwork([2,2,1])\n",
    "    X = np.array([[0, 0],\n",
    "                  [0, 1],\n",
    "                  [1, 0],\n",
    "                  [1, 1]])\n",
    "    y = np.array([0, 1, 1, 0])\n",
    "#     X = np.array([[-1, -1],\n",
    "#                   [-1, 1],\n",
    "#                   [1, -1],\n",
    "#                   [1, 1]])\n",
    "#     y = np.array([0, 1, 1, 0])\n",
    "    for e in X:\n",
    "        print(e,nn.predict(e))\n",
    "    nn.fit(X, y)\n",
    "    for e in X:\n",
    "        print(e,nn.predict(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.29958433,  0.6063289 , -0.36245242],\n",
       "       [ 0.70545008, -0.63699388, -0.98934633],\n",
       "       [ 0.74614418,  0.41442409, -0.61445719]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2*np.random.random((3,3))-1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class NeuralNetwork_tanh:\n",
    "\n",
    "    def __init__(self, layers):\n",
    "        self.activation = tanh\n",
    "        self.activation_prime = tanh_prime\n",
    "#         self.activation = linear\n",
    "#         self.activation_prime = linear_prime\n",
    "#         self.activation = sigmoid\n",
    "#         self.activation_prime = sigmoid_prime\n",
    "        \n",
    "        # Set weights\n",
    "        self.weights = []\n",
    "        \n",
    "        # layers[0] количество входов\n",
    "        \n",
    "        # layers = [2,2,1]\n",
    "        # range of weight values (-1,1)\n",
    "        # input and hidden layers - random((2+1, 2+1)) : 3 x 3\n",
    "        \n",
    "        for i in range(1, len(layers) - 1):\n",
    "            r = 2*np.random.random((layers[i-1] + 1, layers[i] + 1)) -1\n",
    "            self.weights.append(r)\n",
    "            print(r)\n",
    "        # output layer - random((2+1, 1)) : 3 x 1\n",
    "        r = 2*np.random.random( (layers[i] + 1, layers[i+1])) - 1\n",
    "        print(r)\n",
    "        self.weights.append(r)\n",
    "\n",
    "    def fit(self, X, y, learning_rate=0.2, epochs=100000):\n",
    "        # Add column of ones to X\n",
    "        # This is to add the bias unit to the input layer\n",
    "        ones = np.atleast_2d(np.ones(X.shape[0]))\n",
    "        X = np.concatenate((ones.T, X), axis=1)\n",
    "        errX = np.arange(epochs)\n",
    "        errY = []\n",
    "        for k in range(epochs):\n",
    "            i = np.random.randint(X.shape[0])\n",
    "            a = [X[i]]\n",
    "            \n",
    "            for l in range(len(self.weights)):\n",
    "                    dot_value = np.dot(a[l], self.weights[l])\n",
    "                    activation = self.activation(dot_value)\n",
    "                    a.append(activation)\n",
    "            # output layer\n",
    "            \n",
    "            error = y[i] - a[-1]\n",
    "            deltas = [error * self.activation_prime(a[-1])]\n",
    "            errY.append(error)\n",
    "            # we need to begin at the second to last layer \n",
    "            # (a layer before the output layer)\n",
    "            for l in range(len(a) - 2, 0, -1): \n",
    "                deltas.append(deltas[-1].dot(self.weights[l].T)*self.activation_prime(a[l]))\n",
    "\n",
    "            # reverse\n",
    "            # [level3(output)->level2(hidden)]  => [level2(hidden)->level3(output)]\n",
    "            deltas.reverse()\n",
    "\n",
    "            # backpropagation\n",
    "            # 1. Multiply its output delta and input activation \n",
    "            #    to get the gradient of the weight.\n",
    "            # 2. Subtract a ratio (percentage) of the gradient from the weight.\n",
    "            for i in range(len(self.weights)):\n",
    "                layer = np.atleast_2d(a[i])\n",
    "                delta = np.atleast_2d(deltas[i])\n",
    "                self.weights[i] += learning_rate * layer.T.dot(delta)\n",
    "\n",
    "            if k % 10000 == 0: \n",
    "                print('epochs:', k)\n",
    "        plt.plot(errX, errY, color='red')\n",
    "\n",
    "    def predict(self, x): \n",
    "    \n",
    "        a = np.concatenate((np.ones(1).T, np.array(x)))      \n",
    "\n",
    "        for l in range(0, len(self.weights)):\n",
    "            a = self.activation(np.dot(a, self.weights[l]))\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork_linear:\n",
    "\n",
    "    def __init__(self, layers):\n",
    "        \n",
    "#         self.activation = tanh\n",
    "#         self.activation_prime = tanh_prime\n",
    "        self.activation = linear\n",
    "        self.activation_prime = linear_prime\n",
    "#         self.activation = sigmoid\n",
    "#         self.activation_prime = sigmoid_prime\n",
    "        \n",
    "        # Set weights\n",
    "        self.weights = []\n",
    "        # layers = [2,2,1]\n",
    "        # range of weight values (-1,1)\n",
    "        # input and hidden layers - random((2+1, 2+1)) : 3 x 3\n",
    "        \n",
    "        for i in range(1, len(layers) - 1):\n",
    "            r = 2*np.random.random((layers[i-1] + 1, layers[i] + 1)) -1\n",
    "            self.weights.append(r)\n",
    "            print(r)\n",
    "        # output layer - random((2+1, 1)) : 3 x 1\n",
    "        r = 2*np.random.random( (layers[i] + 1, layers[i+1])) - 1\n",
    "        print(r)\n",
    "        self.weights.append(r)\n",
    "\n",
    "    def fit(self, X, y, learning_rate=0.2, epochs=100000):\n",
    "        # Add column of ones to X\n",
    "        # This is to add the bias unit to the input layer\n",
    "        ones = np.atleast_2d(np.ones(X.shape[0]))\n",
    "        X = np.concatenate((ones.T, X), axis=1)\n",
    "         \n",
    "        for k in range(epochs):\n",
    "            i = np.random.randint(X.shape[0])\n",
    "            a = [X[i]]\n",
    "            \n",
    "            for l in range(len(self.weights)):\n",
    "                    dot_value = np.dot(a[l], self.weights[l])\n",
    "                    activation = self.activation(dot_value)\n",
    "                    a.append(activation)\n",
    "            # output layer\n",
    "            \n",
    "            error = y[i] - a[-1]\n",
    "            deltas = [error * self.activation_prime(a[-1])]\n",
    "\n",
    "            # we need to begin at the second to last layer \n",
    "            # (a layer before the output layer)\n",
    "            for l in range(len(a) - 2, 0, -1): \n",
    "                deltas.append(deltas[-1].dot(self.weights[l].T)*self.activation_prime(a[l]))\n",
    "\n",
    "            # reverse\n",
    "            # [level3(output)->level2(hidden)]  => [level2(hidden)->level3(output)]\n",
    "            deltas.reverse()\n",
    "\n",
    "            # backpropagation\n",
    "            # 1. Multiply its output delta and input activation \n",
    "            #    to get the gradient of the weight.\n",
    "            # 2. Subtract a ratio (percentage) of the gradient from the weight.\n",
    "            for i in range(len(self.weights)):\n",
    "                layer = np.atleast_2d(a[i])\n",
    "                delta = np.atleast_2d(deltas[i])\n",
    "                self.weights[i] += learning_rate * layer.T.dot(delta)\n",
    "\n",
    "            if k % 10000 == 0: \n",
    "                print('epochs:', k)\n",
    "\n",
    "    def predict(self, x): \n",
    "    \n",
    "        a = np.concatenate((np.ones(1).T, np.array(x)))      \n",
    "\n",
    "        for l in range(0, len(self.weights)):\n",
    "            a = self.activation(np.dot(a, self.weights[l]))\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork_sigmoid:\n",
    "\n",
    "    def __init__(self, layers):\n",
    "        \n",
    "#         self.activation = tanh\n",
    "#         self.activation_prime = tanh_prime\n",
    "#        self.activation = linear\n",
    "#        self.activation_prime = linear_prime\n",
    "        self.activation = sigmoid\n",
    "        self.activation_prime = sigmoid_prime\n",
    "        \n",
    "        # Set weights\n",
    "        self.weights = []\n",
    "        # layers = [2,2,1]\n",
    "        # range of weight values (-1,1)\n",
    "        # input and hidden layers - random((2+1, 2+1)) : 3 x 3\n",
    "        \n",
    "        for i in range(1, len(layers) - 1):\n",
    "            r = 2*np.random.random((layers[i-1] + 1, layers[i] + 1)) -1\n",
    "            self.weights.append(r)\n",
    "            print(r)\n",
    "        # output layer - random((2+1, 1)) : 3 x 1\n",
    "        r = 2*np.random.random( (layers[i] + 1, layers[i+1])) - 1\n",
    "        print(r)\n",
    "        self.weights.append(r)\n",
    "\n",
    "    def fit(self, X, y, learning_rate=0.2, epochs=100000):\n",
    "        # Add column of ones to X\n",
    "        # This is to add the bias unit to the input layer\n",
    "        ones = np.atleast_2d(np.ones(X.shape[0]))\n",
    "        X = np.concatenate((ones.T, X), axis=1)\n",
    "         \n",
    "        for k in range(epochs):\n",
    "            i = np.random.randint(X.shape[0])\n",
    "            a = [X[i]]\n",
    "            \n",
    "            for l in range(len(self.weights)):\n",
    "                    dot_value = np.dot(a[l], self.weights[l])\n",
    "                    activation = self.activation(dot_value)\n",
    "                    a.append(activation)\n",
    "            # output layer\n",
    "            \n",
    "            error = y[i] - a[-1]\n",
    "            deltas = [error * self.activation_prime(a[-1])]\n",
    "\n",
    "            # we need to begin at the second to last layer \n",
    "            # (a layer before the output layer)\n",
    "            for l in range(len(a) - 2, 0, -1): \n",
    "                deltas.append(deltas[-1].dot(self.weights[l].T)*self.activation_prime(a[l]))\n",
    "\n",
    "            # reverse\n",
    "            # [level3(output)->level2(hidden)]  => [level2(hidden)->level3(output)]\n",
    "            deltas.reverse()\n",
    "\n",
    "            # backpropagation\n",
    "            # 1. Multiply its output delta and input activation \n",
    "            #    to get the gradient of the weight.\n",
    "            # 2. Subtract a ratio (percentage) of the gradient from the weight.\n",
    "            for i in range(len(self.weights)):\n",
    "                layer = np.atleast_2d(a[i])\n",
    "                delta = np.atleast_2d(deltas[i])\n",
    "                self.weights[i] += learning_rate * layer.T.dot(delta)\n",
    "\n",
    "            if k % 10000 == 0: \n",
    "                print('epochs:', k)\n",
    "        \n",
    "\n",
    "    def predict(self, x): \n",
    "    \n",
    "        a = np.concatenate((np.ones(1).T, np.array(x)))      \n",
    "\n",
    "        for l in range(0, len(self.weights)):\n",
    "            a = self.activation(np.dot(a, self.weights[l]))\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.91977576  0.69447723  0.12548641]\n",
      " [ 0.10514921 -0.64120073 -0.6365833 ]\n",
      " [-0.62532599  0.15172656  0.78185524]]\n",
      "[[-0.9243606 ]\n",
      " [-0.78292444]\n",
      " [ 0.36776639]]\n",
      "[0 0] [-0.79884223]\n",
      "[0 1] [-0.49248491]\n",
      "[1 0] [-0.72979927]\n",
      "[1 1] [-0.39014534]\n",
      "[[1. 0. 0.]\n",
      " [1. 0. 1.]\n",
      " [1. 1. 0.]\n",
      " [1. 1. 1.]]\n",
      "epochs: 0\n",
      "epochs: 10000\n",
      "epochs: 20000\n",
      "epochs: 30000\n",
      "epochs: 40000\n",
      "epochs: 50000\n",
      "epochs: 60000\n",
      "epochs: 70000\n",
      "epochs: 80000\n",
      "epochs: 90000\n",
      "[0 0] [9.76845525e-06]\n",
      "[0 1] [0.99616232]\n",
      "[1 0] [0.99718449]\n",
      "[1 1] [-1.29402791e-05]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    nn = NeuralNetwork([2,2,1])\n",
    "    X = np.array([[0, 0],\n",
    "                  [0, 1],\n",
    "                  [1, 0],\n",
    "                  [1, 1]])\n",
    "    y = np.array([0, 1, 1, 0])\n",
    "#     X = np.array([[-1, -1],\n",
    "#                   [-1, 1],\n",
    "#                   [1, -1],\n",
    "#                   [1, 1]])\n",
    "#     y = np.array([0, 1, 1, 0])\n",
    "    for e in X:\n",
    "        print(e,nn.predict(e))\n",
    "    nn.fit(X, y)\n",
    "    for e in X:\n",
    "        print(e,nn.predict(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
